# PI Report: Groove Hub AI Assistant

**Author:** Joaquin Dario Olivero
**Date:** February 2026
**Project:** Groove Hub CLI

---

## 1. Architecture Vision
Groove Hub is designed as a modular **Service-Agent-Schema** microservice, moving away from simple linear scripting to a robust object-oriented architecture. The primary goal was to decouple the business logic from the probabilistic nature of the Large Language Model (LLM).

### Core Components:
* **The Agent (State Manager):** Manages the conversation history and orchestrates the flow between the user and the LLM. It implements a short-term memory mechanism to maintain context.
* **The Schema Layer (Validation):** Uses **Pydantic** to enforce a strict contract. The LLM is constrained to output JSON. If the model generates a non-compliant structure or invents an invalid category, the system intercepts the error before it reaches the user.
* **The Service Layer (Abstraction):** Acts as a chameleon adapter. It automatically detects available API keys (OpenAI or Groq) and instantiates the correct client, allowing for seamless transition between development (Groq/Llama3) and production (OpenAI/GPT-4).

## 2. Prompting Techniques & Rationale
To ensure high reliability and reduce hallucinations, we implemented a **Few-Shot Chain-of-Thought (CoT)** strategy.

1.  **Context Setting:** The System Prompt explicitly defines the persona ("Groov") and resolves domain ambiguity (e.g., instructing the model that "drums" refers to musical instruments, not containers).
2.  **Reasoning-First Approach (CoT):** We instruct the model to generate a `"reasoning"` field *before* the final `"answer"`.
    * *Why?* This forces the LLM to "think" and classify the user's intent logically before committing to a response. This significantly improves the accuracy of the `intent` classification.
3.  **Few-Shot Learning:** We provide three distinct examples of Input -> Reasoning -> Output within the prompt.
    * *Why?* This guides the model on the expected tone, JSON structure, and how to handle edge cases (like off-topic questions).

## 3. Metrics Summary
Observability is implemented via a custom `MetricsTracker` using **Tiktoken** for precise accounting.

* **Latency:** Measures the raw generation time (excluding local validation).
* **Token Usage:** Tracks Prompt (Input) and Completion (Output) tokens separately.
* **Estimated Cost:** Calculates the USD cost per query based on model pricing tiers.
* **Confidence Score:** A self-assessed metric generated by the model regarding the quality of its advice.

### Sample Log Entry
```json
{
  "timestamp": "2026-02-16T16:06:55.711269",
  "query_preview": "Bater√≠as para Jazz, que me recomiendas?",
  "metrics": {
    "latency_ms": 1101,
    "cost_usd": 0.000391,
    "total_tokens": 201
  }
}
```

## 4. Challenges Faced
* **Strict JSON Enforcement:** LLMs occasionally add conversational text outside the JSON block. We solved this by implementing a parsing retry mechanism and strict prompt instructions.
* **API Compatibility:** Switching between Groq and OpenAI required normalizing the API calls, as their SDKs handle `base_url` and model names differently.
* **Ambiguity Handling:** Initial tests showed confusion with terms like "platillos" (cymbals vs. dishes). This was resolved by refining the "Context Setting" in the prompt.

## 5. Potential Improvements
* **RAG Integration (Retrieval-Augmented Generation):** Currently, the stock is hallucinated based on training data. Connecting the agent to a real Vector Database with actual inventory would allow for factual sales.
* **Web Interface:** Migrating the CLI to a FastAPI + React architecture to support visual catalogs.
* **Async Processing:** Implementing `asyncio` to handle multiple user requests concurrently for better scalability.